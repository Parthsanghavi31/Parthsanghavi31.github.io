---
---

@string{aps = {American Physical Society,}}



@article{
 kausik2023tom,
 author = {Yecheng Jason Ma* and Kausik Sivakumar* and Jason Yen and Osbert Bastani and Dinesh Jayaraman},
 journal = {L4DC},
 title = {Learning Policy-Aware Models for Model-Based Reinforcement Learning via Transition Occupancy Matching},
 abstract={Standard model-based reinforcement learning (MBRL) approaches fit a transition model of the environment to all past experience, but this wastes model capacity on data that is irrelevant for policy improvement. We instead propose a new ``transition occupancy matching'' (TOM) objective for MBRL model learning: a good environment model has the property that the current policy experiences the same distribution of transitions, whether deployed in the real environment or inside the model. We derive TOM directly from a novel lower bound on the standard reinforcement learning objective. To optimize TOM, we show how to reduce it to a form of importance weighted maximum-likelihood estimation, where the automatically computed importance weights identify policy-relevant past experiences from a replay buffer, enabling stable optimization. TOM thus offers a plug-and-play model learning sub-routine that is compatible with any backbone MBRL algorithm. On various Mujoco continuous robotic control tasks, we show that TOM successfully focuses model learning on policy-relevant experience and drives policies faster to higher task rewards than alternative model learning approaches},
 year = {2023},
 selected={true},
 abbr={L4DC},
 pdf={https://www.seas.upenn.edu/~jasonyma/materials/L4DC-2023.pdf},
 openreview={https://openreview.net/pdf?id=DCqOe3-yGT}
}


@inproceedings{
ma2022policy,
title={Policy Aware Model Learning via Transition Occupancy Matching},
author={Yecheng Jason Ma* and Kausik Sivakumar* and Osbert Bastani and Dinesh Jayaraman},
abstract={Model-based reinforcement learning (MBRL) is an effective paradigm for sample-efficient policy learning. The pre-dominant MBRL strategy iteratively learns the dynamics model by performing maximum likelihood (MLE) on the entire replay buffer and trains the policy using fictitious transitions from the learned model. Given that not all transitions in the replay buffer are equally informative about the task or the policy's current progress, this MLE strategy cannot be optimal and bears no clear relation to the standard RL objective. In this work, we propose Transition Occupancy Matching (TOM), a policy-aware model learning algorithm that maximizes a lower bound on the standard RL objective. TOM learns a policy-aware dynamics model by minimizing an -divergence between the distribution of transitions that the current policy visits in the real environment and in the learned model; then, the policy can be updated using any pre-existing RL algorithm with log-transformed reward. TOM's practical implementation builds on tools from dual reinforcement learning and learns the optimal transition occupancy ratio between the current policy and the replay buffer; leveraging this ratio as importance weights, TOM amounts to performing MLE model learning on the correct, policy aware transition distribution. Crucially, TOM is a model learning sub-routine and is compatible with any backbone MBRL algorithm that implements MLE-based model learning. On the standard set of Mujoco locomotion tasks, we find TOM improves the learning speed of a standard MBRL algorithm and can reach the same asymptotic performance with as much as 50% fewer samples},
pdf={https://openreview.net/pdf?id=UW-VQXWuuWB},
booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022},
year={2022},
abbr={NeurIPS},
url={https://openreview.net/forum?id=UW-VQXWuuWB},
selected={true}
}